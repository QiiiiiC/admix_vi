{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "smoke_test = ('CI' in os.environ)\n",
    "assert pyro.__version__.startswith('1.9.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a simple model: choose a coin. Ground truth is the probability of heads is drawn from a beta(10,10). Then we do variational inference on this probability f by setting the variational family beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 7.213503122329712\n",
      "Step 100: Loss = 7.092877984046936\n",
      "Step 200: Loss = 7.196681976318359\n",
      "Step 300: Loss = 7.0459043979644775\n",
      "Step 400: Loss = 7.070078015327454\n",
      "Step 500: Loss = 7.090681076049805\n",
      "Step 600: Loss = 7.023242712020874\n",
      "Step 700: Loss = 7.077595472335815\n",
      "Step 800: Loss = 7.107882022857666\n",
      "Step 900: Loss = 6.973265290260315\n",
      "Step 1000: Loss = 7.118948936462402\n",
      "Step 1100: Loss = 6.958721160888672\n",
      "Step 1200: Loss = 7.103104591369629\n",
      "Step 1300: Loss = 7.0642160177230835\n",
      "Step 1400: Loss = 7.059703230857849\n",
      "Step 1500: Loss = 7.075753092765808\n",
      "Step 1600: Loss = 7.032650589942932\n",
      "Step 1700: Loss = 7.054135680198669\n",
      "Step 1800: Loss = 7.046597838401794\n",
      "Step 1900: Loss = 7.0928120613098145\n",
      "\n",
      "Based on the data and our prior belief, the fairness of the coin is 0.535 +- 0.090\n"
     ]
    }
   ],
   "source": [
    "def model(data):\n",
    "    # define the hyperparameters that control the Beta prior\n",
    "\n",
    "    f1 = pyro.sample(\"latent_fairness_1\", dist.Beta(10, 10))\n",
    "    # loop over the observed data\n",
    "    for i in range(len(data)):\n",
    "        # observe datapoint i using the Bernoulli\n",
    "        # likelihood Bernoulli(f)\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f1), obs=data[i])\n",
    "\n",
    "def guide_custome(model):\n",
    "\n",
    "    # register the two variational parameters with Pyro.\n",
    "    alpha_q1 = pyro.param(\"alpha_q1\", torch.tensor(15.0),\n",
    "                         constraint=dist.constraints.positive)\n",
    "    beta_q1 = pyro.param(\"beta_q1\", torch.tensor(15.0),\n",
    "                        constraint=dist.constraints.positive)\n",
    "\n",
    "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
    "    pyro.sample(\"latent_fairness_1\", dist.Beta(alpha_q1, beta_q1))\n",
    "\n",
    "\n",
    "# setup the optimizer\n",
    "adam_params = {\"lr\": 0.0005}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(model, guide_custome, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "data = []\n",
    "for _ in range(6):\n",
    "    data.append(torch.tensor(1.0))\n",
    "for _ in range(4):\n",
    "    data.append(torch.tensor(0.0))\n",
    "\n",
    "\n",
    "# do gradient steps\n",
    "n_steps = 2000\n",
    "losses = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    loss = svi.step(data)\n",
    "    losses.append(loss)\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}: Loss = {loss}\")\n",
    "\n",
    "\n",
    "alpha_q1 = pyro.param(\"alpha_q1\").item()\n",
    "beta_q1 = pyro.param(\"beta_q1\").item()\n",
    "\n",
    "inferred_mean = alpha_q1 / (alpha_q1 + beta_q1)\n",
    "# compute inferred standard deviation\n",
    "factor = beta_q1 / (alpha_q1 * (1.0 + alpha_q1 + beta_q1))\n",
    "inferred_std = inferred_mean * math.sqrt(factor)\n",
    "\n",
    "print(\"\\nBased on the data and our prior belief, the fairness \" +\n",
    "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the autoguide function autoDiagonalNormal to see if ADVI can solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 6.1860562562942505\n",
      "Step 100: Loss = 8.403323292732239\n",
      "Step 200: Loss = 7.725732088088989\n",
      "Step 300: Loss = 8.254777193069458\n",
      "Step 400: Loss = 8.21003532409668\n",
      "Step 500: Loss = 8.004858374595642\n",
      "Step 600: Loss = 6.89823317527771\n",
      "Step 700: Loss = 8.006877779960632\n",
      "Step 800: Loss = 6.200948596000671\n",
      "Step 900: Loss = 7.560607552528381\n",
      "Step 1000: Loss = 7.92170774936676\n",
      "Step 1100: Loss = 7.88515031337738\n",
      "Step 1200: Loss = 7.842462420463562\n",
      "Step 1300: Loss = 7.314276099205017\n",
      "Step 1400: Loss = 7.757480263710022\n",
      "Step 1500: Loss = 7.704437255859375\n",
      "Step 1600: Loss = 7.552847504615784\n",
      "Step 1700: Loss = 7.633719563484192\n",
      "Step 1800: Loss = 7.615230202674866\n",
      "Step 1900: Loss = 7.443209886550903\n",
      "alpha_q1 = 16.04521942138672\n",
      "beta_q1 = 13.945406913757324\n",
      "AutoDiagonalNormal.loc = [0.14202186]\n",
      "AutoDiagonalNormal.scale = [0.22710498]\n"
     ]
    }
   ],
   "source": [
    "auto_guide = AutoDiagonalNormal(model)\n",
    "svi = SVI(model, auto_guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "n_steps = 2000\n",
    "losses = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    loss = svi.step(data)\n",
    "    losses.append(loss)\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}: Loss = {loss}\")\n",
    "\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print(f\"{name} = {value.detach().cpu().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the mannual implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the inferred loc and scale is far from true. What's worse is that if we change the model to include two parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)):\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# observe datapoint i using the Bernoulli\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# likelihood Bernoulli(f)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         pyro\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i), dist\u001b[38;5;241m.\u001b[39mBernoulli(f1\u001b[38;5;241m*\u001b[39mf2), obs\u001b[38;5;241m=\u001b[39mdata[i])\n\u001b[0;32m---> 13\u001b[0m \u001b[43mpyro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_distributions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/msprime-env/lib/python3.12/site-packages/pyro/infer/inspect.py:645\u001b[0m, in \u001b[0;36mrender_model\u001b[0;34m(model, model_args, model_kwargs, filename, render_distributions, render_params, render_deterministic)\u001b[0m\n\u001b[1;32m    642\u001b[0m         model_args \u001b[38;5;241m=\u001b[39m [model_args] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(model_kwargs)\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(model_kwargs)\n\u001b[1;32m    644\u001b[0m     relations \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 645\u001b[0m         \u001b[43mget_model_relations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_deterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender_deterministic\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model_args, model_kwargs)\n\u001b[1;32m    649\u001b[0m     ]\n\u001b[1;32m    651\u001b[0m \u001b[38;5;66;03m# Get graph specifications.\u001b[39;00m\n\u001b[1;32m    652\u001b[0m graph_specs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    653\u001b[0m     generate_graph_specification(r, render_params\u001b[38;5;241m=\u001b[39mrender_params) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m relations\n\u001b[1;32m    654\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/envs/msprime-env/lib/python3.12/site-packages/pyro/infer/inspect.py:297\u001b[0m, in \u001b[0;36mget_model_relations\u001b[0;34m(model, model_args, model_kwargs, include_deterministic)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_args, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_kwargs, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mfork_rng(), torch\u001b[38;5;241m.\u001b[39mno_grad(), pyro\u001b[38;5;241m.\u001b[39mvalidation_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def model(data):\n",
    "    # define the hyperparameters that control the Beta prior\n",
    "\n",
    "    # sample f from the Beta prior\n",
    "    f1 = pyro.sample(\"latent_fairness_1\", dist.Beta(10, 10))\n",
    "    f2 = pyro.sample(\"latent_fairness_2\", dist.Beta(15, 10))\n",
    "    # loop over the observed data\n",
    "    for i in range(len(data)):\n",
    "        # observe datapoint i using the Bernoulli\n",
    "        # likelihood Bernoulli(f)\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f1*f2), obs=data[i])\n",
    "\n",
    "pyro.render_model(model, model_args=(data), render_distributions=True, render_params=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 8.728413105010986\n",
      "Step 100: Loss = 8.319986701011658\n",
      "Step 200: Loss = 9.983762741088867\n",
      "Step 300: Loss = 9.776553630828857\n",
      "Step 400: Loss = 5.533115863800049\n",
      "Step 500: Loss = 8.818577289581299\n",
      "Step 600: Loss = 9.619333267211914\n",
      "Step 700: Loss = 8.925556898117065\n",
      "Step 800: Loss = 8.452331185340881\n",
      "Step 900: Loss = 7.522561430931091\n",
      "Step 1000: Loss = 8.288254022598267\n",
      "Step 1100: Loss = 8.806099653244019\n",
      "Step 1200: Loss = 8.526350259780884\n",
      "Step 1300: Loss = 8.105169773101807\n",
      "Step 1400: Loss = 8.950329542160034\n",
      "Step 1500: Loss = 8.330858945846558\n",
      "Step 1600: Loss = 8.433443069458008\n",
      "Step 1700: Loss = 8.39637565612793\n",
      "Step 1800: Loss = 8.413408160209656\n",
      "Step 1900: Loss = 8.325928211212158\n",
      "\n",
      "Based on the data and our prior belief, the fairness of the coin is 0.530 +- 0.090\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def guide_custome(model):\n",
    "\n",
    "    # register the two variational parameters with Pyro.\n",
    "    alpha_q1 = pyro.param(\"alpha_q1\", torch.tensor(15.0),\n",
    "                         constraint=dist.constraints.positive)\n",
    "    beta_q1 = pyro.param(\"beta_q1\", torch.tensor(15.0),\n",
    "                        constraint=dist.constraints.positive)\n",
    "    alpha_q2 = pyro.param(\"alpha_q2\", torch.tensor(15.0),\n",
    "                         constraint=dist.constraints.positive)\n",
    "    beta_q2 = pyro.param(\"beta_q2\", torch.tensor(15.0),\n",
    "                        constraint=dist.constraints.positive)\n",
    "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
    "    pyro.sample(\"latent_fairness_1\", dist.Beta(alpha_q1, beta_q1))\n",
    "    pyro.sample(\"latent_fairness_2\", dist.Beta(alpha_q2, beta_q2))\n",
    "\n",
    "# setup the optimizer\n",
    "adam_params = {\"lr\": 0.0005}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(model, guide_custome, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "data = []\n",
    "for _ in range(6):\n",
    "    data.append(torch.tensor(1.0))\n",
    "for _ in range(4):\n",
    "    data.append(torch.tensor(0.0))\n",
    "\n",
    "\n",
    "# do gradient steps\n",
    "n_steps = 2000\n",
    "losses = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    loss = svi.step(data)\n",
    "    losses.append(loss)\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}: Loss = {loss}\")\n",
    "\n",
    "\n",
    "alpha_q = pyro.param(\"alpha_q\").item()\n",
    "beta_q = pyro.param(\"beta_q\").item()\n",
    "\n",
    "inferred_mean = alpha_q / (alpha_q + beta_q)\n",
    "# compute inferred standard deviation\n",
    "factor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q))\n",
    "inferred_std = inferred_mean * math.sqrt(factor)\n",
    "\n",
    "print(\"\\nBased on the data and our prior belief, the fairness \" +\n",
    "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_q = 15.89972972869873\n",
      "beta_q = 14.127716064453125\n",
      "AutoDiagonalNormal.loc = [0.14392696]\n",
      "AutoDiagonalNormal.scale = [0.36030772]\n",
      "alpha_q1 = 17.080364227294922\n",
      "beta_q1 = 12.374051094055176\n",
      "alpha_q2 = 20.10841178894043\n",
      "beta_q2 = 10.899086952209473\n"
     ]
    }
   ],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(f\"{name} = {value.detach().cpu().numpy()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msprime-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
