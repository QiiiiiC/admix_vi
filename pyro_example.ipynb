{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "smoke_test = ('CI' in os.environ)\n",
    "assert pyro.__version__.startswith('1.9.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a simple model: choose a coin. Ground truth is the probability of heads is drawn from a beta(10,10). Then we do variational inference on this probability f by setting the variational family beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 7.079536557197571\n",
      "Step 100: Loss = 7.046950459480286\n",
      "Step 200: Loss = 7.051387071609497\n",
      "Step 300: Loss = 7.092971563339233\n",
      "Step 400: Loss = 7.081258296966553\n",
      "Step 500: Loss = 7.070039987564087\n",
      "Step 600: Loss = 7.065178036689758\n",
      "Step 700: Loss = 7.057203531265259\n",
      "Step 800: Loss = 7.071073770523071\n",
      "Step 900: Loss = 7.048192739486694\n",
      "Step 1000: Loss = 7.102862358093262\n",
      "Step 1100: Loss = 7.074877858161926\n",
      "Step 1200: Loss = 7.089845061302185\n",
      "Step 1300: Loss = 7.05859249830246\n",
      "Step 1400: Loss = 7.069109559059143\n",
      "Step 1500: Loss = 7.071178674697876\n",
      "Step 1600: Loss = 7.020928859710693\n",
      "Step 1700: Loss = 7.037477970123291\n",
      "Step 1800: Loss = 7.041293025016785\n",
      "Step 1900: Loss = 7.112540245056152\n",
      "\n",
      "Based on the data and our prior belief, the fairness of the coin is 0.541 +- 0.090\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "def model(data):\n",
    "    # define the hyperparameters that control the Beta prior\n",
    "\n",
    "    f1 = pyro.sample(\"latent_fairness_1\", dist.Beta(10, 10))\n",
    "    # loop over the observed data\n",
    "    for i in range(len(data)):\n",
    "        # observe datapoint i using the Bernoulli\n",
    "        # likelihood Bernoulli(f)\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f1), obs=data[i])\n",
    "\n",
    "def guide_custom(data):\n",
    "\n",
    "    # register the two variational parameters with Pyro.\n",
    "    alpha_q1 = pyro.param(\"alpha_q1\", torch.tensor(15.0),\n",
    "                         constraint=dist.constraints.positive)\n",
    "    beta_q1 = pyro.param(\"beta_q1\", torch.tensor(15.0),\n",
    "                        constraint=dist.constraints.positive)\n",
    "\n",
    "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
    "    pyro.sample(\"latent_fairness_1\", dist.Beta(alpha_q1, beta_q1))\n",
    "\n",
    "\n",
    "# setup the optimizer\n",
    "adam_params = {\"lr\": 0.0005}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(model, guide_custom, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "data = []\n",
    "for _ in range(6):\n",
    "    data.append(torch.tensor(1.0))\n",
    "for _ in range(4):\n",
    "    data.append(torch.tensor(0.0))\n",
    "\n",
    "\n",
    "# do gradient steps\n",
    "n_steps = 2000\n",
    "losses = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    loss = svi.step(data)\n",
    "    losses.append(loss)\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}: Loss = {loss}\")\n",
    "\n",
    "\n",
    "alpha_q1 = pyro.param(\"alpha_q1\").item()\n",
    "beta_q1 = pyro.param(\"beta_q1\").item()\n",
    "\n",
    "inferred_mean = alpha_q1 / (alpha_q1 + beta_q1)\n",
    "# compute inferred standard deviation\n",
    "factor = beta_q1 / (alpha_q1 * (1.0 + alpha_q1 + beta_q1))\n",
    "inferred_std = inferred_mean * math.sqrt(factor)\n",
    "\n",
    "print(\"\\nBased on the data and our prior belief, the fairness \" +\n",
    "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the autoguide function autoDiagonalNormal to see if ADVI can solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 7.3824931383132935\n",
      "Step 100: Loss = 7.511573910713196\n",
      "Step 200: Loss = 7.485952734947205\n",
      "Step 300: Loss = 6.924910664558411\n",
      "Step 400: Loss = 7.326046347618103\n",
      "Step 500: Loss = 7.347728729248047\n",
      "Step 600: Loss = 6.904543399810791\n",
      "Step 700: Loss = 7.315403342247009\n",
      "Step 800: Loss = 6.635766625404358\n",
      "Step 900: Loss = 6.842504858970642\n",
      "Step 1000: Loss = 7.215908527374268\n",
      "Step 1100: Loss = 7.257016181945801\n",
      "Step 1200: Loss = 6.8420621156692505\n",
      "Step 1300: Loss = 6.9793747663497925\n",
      "Step 1400: Loss = 7.227752089500427\n",
      "Step 1500: Loss = 7.1300565004348755\n",
      "Step 1600: Loss = 7.210431098937988\n",
      "Step 1700: Loss = 7.1971904039382935\n",
      "Step 1800: Loss = 7.191376566886902\n",
      "Step 1900: Loss = 7.151340126991272\n",
      "alpha_q1 = 16.096538543701172\n",
      "beta_q1 = 13.950069427490234\n",
      "AutoDiagonalNormal.loc = [0.14385241]\n",
      "AutoDiagonalNormal.scale = [0.33450317]\n",
      "mean = 9.970494270324707\n"
     ]
    }
   ],
   "source": [
    "auto_guide = AutoDiagonalNormal(model)\n",
    "svi = SVI(model, auto_guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "n_steps = 2000\n",
    "losses = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    loss = svi.step(data)\n",
    "    losses.append(loss)\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}: Loss = {loss}\")\n",
    "\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print(f\"{name} = {value.detach().cpu().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some other guide that we manually make the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vo22292/anaconda3/envs/admix/lib/python3.9/site-packages/pyro/util.py:288: UserWarning: Found non-auxiliary vars in guide but not model, consider marking these infer={'is_auxiliary': True}:\n",
      "{'x'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 4.732509672641754\n",
      "Step 100: Loss = 6.1552417278289795\n",
      "Step 200: Loss = 9.10539186000824\n",
      "Step 300: Loss = 6.330778419971466\n",
      "Step 400: Loss = 6.159422755241394\n",
      "Step 500: Loss = 8.217960596084595\n",
      "Step 600: Loss = 4.825847387313843\n",
      "Step 700: Loss = 5.2056132555007935\n",
      "Step 800: Loss = 7.444134712219238\n",
      "Step 900: Loss = 7.612884283065796\n",
      "Step 1000: Loss = 4.964548230171204\n",
      "Step 1100: Loss = 6.554319858551025\n",
      "Step 1200: Loss = 6.134634852409363\n",
      "Step 1300: Loss = 6.9302650690078735\n",
      "Step 1400: Loss = 5.252403378486633\n",
      "Step 1500: Loss = 5.8072633147239685\n",
      "Step 1600: Loss = 5.84585964679718\n",
      "Step 1700: Loss = 5.343686878681183\n",
      "Step 1800: Loss = 7.718535780906677\n",
      "Step 1900: Loss = 7.622731328010559\n",
      "alpha_q1 = 16.16783905029297\n",
      "beta_q1 = 13.730469703674316\n",
      "AutoDiagonalNormal.loc = [0.14385241]\n",
      "AutoDiagonalNormal.scale = [0.33450317]\n",
      "mean = nan\n",
      "std = nan\n",
      "loc = 0.1232151985168457\n",
      "scale = 0.6606632471084595\n"
     ]
    }
   ],
   "source": [
    "def guide_custom1(data):\n",
    "    # Define parameters for the variational distribution of x\n",
    "    loc = pyro.param(\"loc\", torch.tensor(0.0))\n",
    "    scale = pyro.param(\"scale\", torch.tensor(1.0), constraint=dist.constraints.positive)\n",
    "    \n",
    "    # Sample x from the variational distribution\n",
    "    x = pyro.sample(\"x\", dist.Normal(loc, scale))\n",
    "    \n",
    "    # Apply the sigmoid transformation to x to get f\n",
    "    f = torch.sigmoid(x)\n",
    "    \n",
    "    # Register f as a variational parameter\n",
    "    pyro.sample(\"latent_fairness_1\", dist.Delta(f))\n",
    "    \n",
    "    \n",
    "svi = SVI(model, guide_custom1, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "n_steps = 2000\n",
    "losses = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    loss = svi.step(data)\n",
    "    losses.append(loss)\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}: Loss = {loss}\")\n",
    "\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print(f\"{name} = {value.detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5308)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc = pyro.param('loc').item()\n",
    "torch.sigmoid(torch.tensor(loc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msprime-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
